\subsubsection{Giới thiệu}
% https://viblo.asia/p/transformers-nguoi-may-bien-hinh-bien-doi-the-gioi-nlp-924lJPOXKPM
Mô hình Sequence-to-Sequence (Seq2Seq) nhận đầu vào là một chuỗi và trả lại đầu ra cũng là một chuỗi. Ví dụ bài toán QA, đầu vào là câu hỏi ``how are you?'' và đầu ra là câu trả lời ``I am good''. Phương pháp truyền thống sử dụng RNN cho cả encoder (phần mã hóa đầu vào) và decoder (phần giải mã đầu vào và trả đầu ra tương ứng)\cite{webpage22}.

Nhưng mô hình này vẫn có nhiều điểm yếu như đã trình bày, điểm yếu đầu tiên là tốc độ huyến luyện rất chậm, do bản chất tuần hoàn của mạng, việc tính toán gradient cho các bước thời gian trước đó cần thực hiện theo trình tự, dẫn đến việc sử dụng CPU thay vì GPU, vốn có khả năng tính toán song song hiệu quả hơn. Việc sử dụng Truncated Backpropagation để cải thiện tốc độ huấn luyện vẫn còn hạn chế. Điểm yếu thứ hai là nó xử lý không tốt với những câu dài do hiện tượng Gradient Vanishing/Exploding như đã đề cập.

Ra đời năm 1997, LSTM và theo sau đó là GRU (2000) có vẻ đã giải quyết phần nào vẫn đề Gradient Vanishing, nhưng cả hai kỹ thuật này lại phức tạp hơn RNN rất nhiều, và hiển nhiên nó cũng train chậm hơn RNN đáng kể.

Vậy có cách nào tận dụng khả năng tính toán song song của GPU để tăng tốc độ học hỏi cho các mô hình ngôn ngữ, đồng thời khắc phục điểm yếu xử lý câu dài không? Transformers chính là câu trả lời. Ý tưởng chính của phần này được lấy từ bài báo Attention is all you need[xxx] xuất bản vào năm 2017 của tác giả Ashish Vaswani tại hội nghị quốc tế hằng năm về hệ thống xử lý thông tin mạng thần kinh nhân tạo (NeurIPS). Tính đến năm 2024, bài báo đã được trích dẫn 100.000 lần, trở thành một trong những bài báo có ảnh hưởng nhất trong lĩnh vực học máy. Bài báo giới thiệu kiến trúc Transformer, một mô hình học máy mới dựa hoàn toàn trên cơ chế chú ý (attention). Transformer đã thay đổi cách thức xây dựng các mô hình học máy cho các nhiệm vụ xử lý ngôn ngữ tự nhiên, đặc biệt là dịch máy.

Điều đặc biệt về Transformer so với mô hình RNN là nó không yêu cầu xử lý các phần tử trong chuỗi một cách tuần tự. Trong khi RNN phải xử lý từng phần tử theo thứ tự từ đầu đến cuối, Transformer có thể xử lý toàn bộ câu ngôn ngữ tự nhiên cùng một lúc. Điều này cho phép Transformer tận dụng tối đa khả năng tính toán song song của GPU và giảm thời gian xử lý đáng kể. Thay vì phải chờ đợi kết quả từ các phần tử trước đó trong chuỗi, Transformer có thể đồng thời xem xét và ánh xạ mối quan hệ giữa tất cả các từ trong câu. Điều này giúp mô hình hiểu được mối quan hệ toàn diện trong câu và tạo ra các biểu diễn ngữ nghĩa phong phú hơn.

% https://arxiv.org/abs/1706.03762
% https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html
\subsubsection{Kiến trúc Tranformer}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{image/tranformer-architecture.png}
    \caption{Kiến trúc của mô hình Tranformer \cite{46201}}
    \label{figure:tranformer-architecture}
\end{figure}

Kiến trúc của Tranformer gồm 2 phần là Encoder bên trái và Decoder bên phải\cite{webpage23}.

\textbf{Encoder}: là tổng hợp xếp chồng lên nhau của 6 layers xác định. Mỗi layer bao gồm 2 layer con (sub-layer) trong nó. Sub-layer đầu tiên là multi-head self-attention mà lát nữa chúng ta sẽ tìm hiểu. Layer thứ 2 đơn thuần chỉ là các fully-connected feed-forward layer. Một lưu ý là chúng ta sẽ sử dụng một kết nối residual ở mỗi sub-layer ngay sau layer normalization. Kiến trúc này có ý tưởng tương tự như mạng resnet trong CNN. Đầu ra của mỗi sub-layer là $LayerNorm(x+Sublayer(x))$ có số chiều là 512 theo như bài viết.

\textbf{Decoder}: Decoder cũng là tổng hợp xếp chồng của 6 layers. Kiến trúc tương tự như các sub-layer ở Encoder ngoại trừ thêm 1 sub-layer thể hiện phân phối attention ở vị trí đầu tiên. Layer này không gì khác so với multi-head self-attention layer ngoại trừ được điều chỉnh để không đưa các từ trong tương lai vào attention. Tại bước thứ $i$ của decoder chúng ta chỉ biết được các từ ở vị trí nhỏ hơn $i$ nên việc điều chỉnh đảm bảo attention chỉ áp dụng cho những từ nhỏ hơn vị trí thứ $i$. Cơ chế residual cũng được áp dụng tương tự như trong Encoder.

Ngoài ra luôn có một bước cộng thêm Positional Encoding vào các đầu vào của encoder và decoder nhằm đưa thêm yếu tố thời gian vào mô hình làm tăng độ chuẩn xác. Đây chỉ đơn thuần là phép cộng vector mã hóa vị trí của từ trong câu với vector biểu diễn từ. Chúng ta có thể mã hóa dưới dạng $[0, 1]$ vector vị trí hoặc sử dụng hàm $sin,cos$ như trong bài báo.

\subsubsection{Cơ chế Attention}
\begin{enumerate}[label=\textbf{\arabic*}]
    \item Scaled Dot-Product Attention:

          Đây chính là một cơ chế self-attention khi mỗi từ có thể điều chỉnh trọng số của nó cho các từ khác trong câu sao cho từ ở vị trí càng gần nó nhất thì trọng số càng lớn và càng xa thì càng nhỏ dần. Sau bước nhúng từ (đi qua embeding layer) ta có đầu vào của encoder và decoder là ma trận $\mathbf{X}$ kích thước $m\times n$, $m,n$ lần lượt là là độ dài câu và số chiều của một vector nhúng từ.

          Sau đó, ma trận $\mathbf{X}$ sẽ được nhân với 3 ma trận trọng số tương ứng $\mathbf{W}_\mathbf{q}$, $\mathbf{W}_\mathbf{k}$, $\mathbf{W}_\mathbf{v}$ chính là những hệ số mà model cần huấn luyện, ta thu được ma trận $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$ (Query, Key và Value). Ma trận Query và Key có tác dụng tính toán ra phân phối score cho các cặp từ. Ma trận Value sẽ dựa trên phân phối score để tính ra vector phân phối xác suất đầu ra. Ba ma trận $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$ chính là 3 mũi tên đầu vào của các Multi-head Attention (bản chất là Self-Attention) trong kiến trúc tổng quát hình x.

          Như đã đề cập, Đầu vào để tính attention sẽ bao gồm ma trận $\mathbf{Q}$ (mỗi dòng của nó là một vector query đại diện cho các từ trong đầu vào), ma trận $\mathbf{K}$ (tương tự như ma trận $\mathbf{Q}$, mỗi dòng là vector key đại diện cho các từ trong đầu vào). Hai ma trận $\mathbf{Q}$, $\mathbf{K}$ được sử dụng để tính attention mà các từ trong câu trả về cho 1 từ cụ thể trong câu. Attention vector sẽ được tính dựa trên trung bình có trọng số của các vector value trong ma trận $\mathbf{V}$ với trọng số attention (được tính từ $\mathbf{Q}$, $\mathbf{K}$).

          Phương trình Attention như sau:
          \begin{align}
              \mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
          \end{align}

          Việc chia cho $d_k$ là số dimension của vector key nhằm mục đích tránh tràn luồng nếu số mũ là quá lớn.
          Trong thực hành chúng ta tính toán hàm attention trên toàn bộ tập các câu truy vấn một cách đồng thời được đóng gói thông qua ma trận $\mathbf{Q}$. Keys và Values cũng được đóng gói cùng nhau thông qua ma trận $\mathbf{K},\mathbf{V}$.

          \begin{figure}[htb]
              \centering
              \includegraphics[width=0.8\textwidth]{image/attention-equation.png}
              \caption{Phương trình Attention thể hiện dưới dạng ma trận \cite{webpage27}}
              \label{figure:attention-equation}
          \end{figure}

    \item Multi-head Attention:

          \begin{figure}[htb]
              \centering
              \includegraphics[width=0.9\textwidth]{image/multi-head-illustration.png}
              \caption{Multi-head Attention minh hoạ dưới dạng ma trận \cite{webpage27}}
              \label{figure:multi-head-illustration}
          \end{figure}

          Như vậy sau quá trình Scale dot production chúng ta sẽ thu được 1 ma trận attention. Các tham số mà model cần tinh chỉnh chính là các ma trận $\mathbf{W}_\mathbf{q},\mathbf{W}_\mathbf{k},\mathbf{W}_\mathbf{v}$. Mỗi quá trình như vậy được gọi là một head của attention. Khi lặp lại quá trình này nhiều lần (trong bài báo là ba heads) ta sẽ thu được quá trình Multi-head Attention.

          Sau khi thu được ba ma trận attention ở đầu ra chúng ta sẽ concatenate các matrix này theo các cột để thu được ma trận tổng hợp multi-head matrix có chiều cao trùng với chiều cao của ma trận đầu vào.
          \begin{align}
              \mathrm{MultiHead}(Q, K, V) & = \mathrm{Concat}(\mathrm{head_1},\mathrm{head_2}, ..., \mathrm{head_h})W^O \\
              %    \mathrm{where} \mathrm{head_i} &= \mathrm{Attention}(QW_Q_i^{\dmodel \times d_q}, KW_K_i^{\dmodel \times d_k}, VW^V_i^{\dmodel \times d_v})\\
              \text{với}~\mathrm{head_i}  & = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
          \end{align}


          Cuối cùng, để trả đầu ra có cùng kích thước với ma trận ban đầu chúng ta chỉ cần nhân với ma trận $\mathbf{W}_\mathbf{0}$ có chiều rộng bằng với chiều rộng của ma trận đầu vào.
\end{enumerate}
