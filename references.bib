@book{Aggarwal2023-zk,
  title     = {Neural networks and deep learning: A textbook},
  author    = {Aggarwal, Charu C},
  publisher = {Springer Nature},
  month     = jun,
  year      = 2023,
  address   = {Cham, Switzerland},
  language  = {en}
}

@book{Aggarwal2022-xj,
  title     = {Machine learning for text},
  author    = {Aggarwal, Charu C},
  publisher = {Springer Nature},
  month     = may,
  year      = 2022,
  address   = {Cham, Switzerland},
  language  = {en}
}

@misc{ciampiconi2023survey,
  title         = {A survey and taxonomy of loss functions in machine learning},
  author        = {Lorenzo Ciampiconi and Adam Elwood and Marco Leonardi and Ashraf Mohamed and Alessandro Rozza},
  year          = 2023,
  eprint        = {2301.05579},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{webpage,
  title        = {An Introduction to Machine Learning},
  howpublished = {\url{https://monkeylearn.com/machine-learning/}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage1,
  title        = {Regression vs. Classification in Machine Learning for Beginners},
  author       = {{John Terra}},
  year         = 2023,
  howpublished = {\url{https://www.simplilearn.com/regression-vs-classification-in-machine-learning-article}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage2,
  title        = {Workflow of a Machine Learning project},
  author       = {{Ayush Pant}},
  year         = 2019,
  howpublished = {\url{https://towardsdatascience.com/workflow-of-a-machine-learning-project-ec1dba419b94}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage3,
  title        = {Derivative of the Softmax Function and the Categorical Cross-Entropy Loss},
  author       = {{Thomas Kurbiel}},
  year         = 2021,
  howpublished = {\url{https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1}},
  note         = {Ngày truy cập: 2023-12-13}
}


  
@misc{webpage5,
  title        = {A Brief Overview of Recurrent Neural Networks (RNN)},
  author       = {{Debasish Kalita}},
  year         = 2023,
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage6,
  title        = {Recurrent Neural Networks cheatsheet},
  author       = {{Afshine Amidi}, {Shervine Amidi}},
  year         = 2019,
  howpublished = {\url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage7,
  title        = {Why Long Short term Memory(LSTM) is called as a Long and Short both type of Memory?},
  author       = {{Cornelius}},
  year         = 2021,
  howpublished = {\url{https://stackoverflow.com/a/66766043/16187830}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage8,
  title        = {Word2Vec Research Paper Explained},
  author       = {{Nikhil Birajdar}},
  year         = 2021,
  howpublished = {\url{https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage9,
  title        = {Word Embedding: Text Analysis: NLP: Part-3: GloVe},
  author       = {{Jaimin Mungalpara}},
  year         = 2021,
  howpublished = {\url{https://medium.com/nerd-for-tech/word-embedding-text-analysis-nlp-part-3-glove-and-fasttext-da21d074237a}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage10,
  title        = {Deep Learning là gì? Tổng quan về Deep Learning từ A-Z},
  author       = {{Hưng Nguyễn}},
  howpublished = {\url{https://vietnix.vn/deep-learning-la-gi/}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage11,
  title        = {Kernel (image processing)},
  year         = 2023,
  howpublished = {\url{https://en.wikipedia.org/wiki/Kernel_(image_processing)}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage12,
  title        = {Viblo - Word Embedding - Tìm hiểu khái niệm cơ bản trong NLP},
  author       = {{Bui Quang Manh}},
  year         = 2020,
  howpublished = {\url{https://viblo.asia/p/word-embedding-tim-hieu-khai-niem-co-ban-trong-nlp-1Je5E93G5nL}},
  note         = {Ngày truy cập: 2023-12-13}
}


@misc{webpage13,
  title        = {Word embedding là gì? Tại sao nó quan trọng?},
  year         = 2019,
  howpublished = {\url{https://trituenhantao.io/kien-thuc/word-embedding-la-gi-tai-sao-no-quan-trong/}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage14,
  title        = {Introduction to FastText Embeddings and its Implication},
  author       = {{Krithika V}, {Analytics Vidhya}},
  year         = 2023,
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2023/01/introduction-to-fasttext-embeddings-and-its-implication/}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage15,
  title        = {200Lab - Google Colab là gì? Hướng dẫn sử dụng Google Colab},
  author       = {{Pum}},
  year         = 2022,
  howpublished = {\url{https://200lab.io/blog/google-colab-la-gi/}},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage16,
  title        = {Bizfly Cloud - TensorFlow là gì? Vai trò của TensorFlow trong sự phát triển của học máy},
  author       = {{Linh Tô}},
  year         = 2021,
  howpublished = {\url{https://bizflycloud.vn/tin-tuc/tensorflow-la-gi-vai-tro-cua-tensorflow-trong-su-phat-trien-cua-hoc-may-20211104172943825.htm}},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage17,
  title        = {GeeksforGeeks - Generating Word Cloud in Python},
  year         = 2021,
  howpublished = {\url{https://www.geeksforgeeks.org/generating-word-cloud-python/ }},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage18,
  title        = {TEK4.VN - Sử dụng Pickle để ghi file nhị phân},
  howpublished = {\url{https://tek4.vn/khoa-hoc/lap-trinh-python-can-ban/su-dung-pickle-de-ghi-file-nhi-phan}},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{data,
  title        = {Toxic Comment Classification Challenge},
  year         = 2018,
  howpublished = {\url{https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data}},
  note         = {Ngày truy cập: 2023-12-21}
}

% copy from github vinai/phobert
@inproceedings{phobert,
title     = {{PhoBERT: Pre-trained language models for Vietnamese}},
author    = {Dat Quoc Nguyen and Anh Tuan Nguyen},
booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
year      = {2020},
pages     = {1037--1042}
}



% copy from google arXiv
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}


% copy from google research
@inproceedings{47751,
      title	= {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author	= {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina N. Toutanova},
      year	= {2018},
      URL	= {https://arxiv.org/abs/1810.04805}
}

@inproceedings{46201,
      title	= {Attention is All You Need},
      author	= {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year	= {2017},
      URL	= {https://arxiv.org/pdf/1706.03762.pdf}
}

@misc{webpage19,
  title        = {BERT Explained: A Complete Guide with Theory and Tutorial},
  author       = {{Samia Khalid}},
  year         = 2019,
  howpublished = {\url{https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage20,
  title        = {Bài 36 - BERT model},
  author       = {{Pham Dinh Khanh}},
  year         = 2020,
  howpublished = {\url{https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage21,
  title        = {BERT, RoBERTa, PhoBERT, BERTweet: Ứng dụng state-of-the-art pre-trained model cho bài toán phân loại văn bản},
  author       = {{Pham Huu Quang}},
  year         = 2020,
  howpublished = {\url{https://viblo.asia/p/bert-roberta-phobert-bertweet-ung-dung-state-of-the-art-pre-trained-model-cho-bai-toan-phan-loai-van-ban-4P856PEWZY3}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage22,
  title        = {Transformers - "Người máy biến hình" biến đổi thế giới NLP},
  author       = {{Nguyet Viet Anh}},
  year         = 2020,
  howpublished = {\url{https://viblo.asia/p/transformers-nguoi-may-bien-hinh-bien-doi-the-gioi-nlp-924lJPOXKPM}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage23,
  title        = {Bài 4 - Attention is all you need},
  author       = {{Pham Dinh Khanh}},
  year         = 2020,
  howpublished = {\url{https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage24,
  title        = {Natural language processing},
  year         = 2023,
  howpublished = {\url{https://en.wikipedia.org/wiki/Natural_language_processing}},
  note         = {Ngày truy cập: 2024-06-18}
}

