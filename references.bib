@book{Aggarwal2023,
  title     = {Neural Networks and Deep Learning: A Textbook},
  isbn      = {9783031296420},
  url       = {http://dx.doi.org/10.1007/978-3-031-29642-0},
  doi       = {10.1007/978-3-031-29642-0},
  publisher = {Springer International Publishing},
  author    = {Aggarwal,  Charu C.},
  year      = {2023}
}

@book{Aggarwal2022,
  title     = {Machine Learning for Text},
  isbn      = {9783030966232},
  url       = {http://dx.doi.org/10.1007/978-3-030-96623-2},
  doi       = {10.1007/978-3-030-96623-2},
  publisher = {Springer International Publishing},
  author    = {Aggarwal,  Charu C.},
  year      = {2022}
}

@misc{ciampiconi2023surveytaxonomylossfunctions,
  title         = {A survey and taxonomy of loss functions in machine learning},
  author        = {Lorenzo Ciampiconi and Adam Elwood and Marco Leonardi and Ashraf Mohamed and Alessandro Rozza},
  year          = {2023},
  eprint        = {2301.05579},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2301.05579}
}

@misc{jigsaw-toxic-comment-classification-challenge,
  author    = {cjadams and Jeffrey Sorensen and Julia Elliott and Lucas Dixon and Mark McDonald and nithum and Will Cukierski},
  title     = {Toxic Comment Classification Challenge},
  publisher = {Kaggle},
  year      = {2017},
  url       = {https://kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge}
}

@misc{nguyen2020phobertpretrainedlanguagemodels,
  title         = {PhoBERT: Pre-trained language models for Vietnamese},
  author        = {Dat Quoc Nguyen and Anh Tuan Nguyen},
  year          = {2020},
  eprint        = {2003.00744},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2003.00744}
}

@misc{devlin2019bertpretrainingdeepbidirectional,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1810.04805}
}

@misc{vaswani2023attentionneed,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@inproceedings{9339521,
  author    = {Dubey, Krishna and Nair, Rahul and Khan, Mohd. Usman and Shaikh, Prof. Sanober},
  booktitle = {2020 Third International Conference on Advances in Electronics, Computers and Communications (ICAECC)},
  title     = {Toxic Comment Detection using LSTM},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1-8},
  keywords  = {Training;Text mining;Toxicology;Computational modeling;Neural networks;Text categorization;Data models;Hate Speech;Word Embedding;Artificial Neural Networks;LSTM;NLP},
  doi       = {10.1109/ICAECC50550.2020.9339521}
}

@article{Sharma2018,
  title     = {Toxic Comment Classification Using Neural Networks and Machine Learning},
  volume    = {5},
  issn      = {2393-8021},
  url       = {http://dx.doi.org/10.17148/IARJSET.2018.597},
  doi       = {10.17148/iarjset.2018.597},
  number    = {9},
  journal   = {IARJSET},
  publisher = {Tejass Publishers},
  author    = {Sharma,  Revati and Patel,  Meetkumar},
  year      = {2018},
  month     = sep,
  pages     = {47-52}
}

@article{app10238631,
  author         = {Maslej-Krešňáková, Viera and Sarnovský, Martin and Butka, Peter and Machová, Kristína},
  title          = {Comparison of Deep Learning Models and Various Text Pre-Processing Techniques for the Toxic Comments Classification},
  journal        = {Applied Sciences},
  volume         = {10},
  year           = {2020},
  number         = {23},
  article-number = {8631},
  url            = {https://www.mdpi.com/2076-3417/10/23/8631},
  issn           = {2076-3417},
  abstract       = {The emergence of anti-social behaviour in online environments presents a serious issue in today’s society. Automatic detection and identification of such behaviour are becoming increasingly important. Modern machine learning and natural language processing methods can provide effective tools to detect different types of anti-social behaviour from the pieces of text. In this work, we present a comparison of various deep learning models used to identify the toxic comments in the Internet discussions. Our main goal was to explore the effect of the data preparation on the model performance. As we worked with the assumption that the use of traditional pre-processing methods may lead to the loss of characteristic traits, specific for toxic content, we compared several popular deep learning and transformer language models. We aimed to analyze the influence of different pre-processing techniques and text representations including standard TF-IDF, pre-trained word embeddings and also explored currently popular transformer models. Experiments were performed on the dataset from the Kaggle Toxic Comment Classification competition, and the best performing model was compared with the similar approaches using standard metrics used in data analysis.},
  doi            = {10.3390/app10238631}
}

@misc{webpage,
  title        = {An Introduction to Machine Learning},
  howpublished = {\url{https://monkeylearn.com/machine-learning/}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage1,
  title        = {Regression vs. Classification in Machine Learning for Beginners},
  author       = {{John Terra}},
  year         = 2023,
  howpublished = {\url{https://www.simplilearn.com/regression-vs-classification-in-machine-learning-article}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage2,
  title        = {Workflow of a Machine Learning project},
  author       = {{Ayush Pant}},
  year         = 2019,
  howpublished = {\url{https://towardsdatascience.com/workflow-of-a-machine-learning-project-ec1dba419b94}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage3,
  title        = {Derivative of the Softmax Function and the Categorical Cross-Entropy Loss},
  author       = {{Thomas Kurbiel}},
  year         = 2021,
  howpublished = {\url{https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage5,
  title        = {A Brief Overview of Recurrent Neural Networks (RNN)},
  author       = {{Debasish Kalita}},
  year         = 2023,
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage6,
  title        = {Recurrent Neural Networks cheatsheet},
  author       = {{Afshine Amidi}, {Shervine Amidi}},
  year         = 2019,
  howpublished = {\url{https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage7,
  title        = {Why Long Short term Memory(LSTM) is called as a Long and Short both type of Memory?},
  author       = {{Cornelius}},
  year         = 2021,
  howpublished = {\url{https://stackoverflow.com/a/66766043/16187830}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage8,
  title        = {Word2Vec Research Paper Explained},
  author       = {{Nikhil Birajdar}},
  year         = 2021,
  howpublished = {\url{https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage9,
  title        = {Word Embedding: Text Analysis: NLP: Part-3: GloVe},
  author       = {{Jaimin Mungalpara}},
  year         = 2021,
  howpublished = {\url{https://medium.com/nerd-for-tech/word-embedding-text-analysis-nlp-part-3-glove-and-fasttext-da21d074237a}},
  note         = {Ngày truy cập: 2023-12-13}
}

  
@misc{webpage10,
  title        = {Deep Learning là gì? Tổng quan về Deep Learning từ A-Z},
  author       = {{Hưng Nguyễn}},
  howpublished = {\url{https://vietnix.vn/deep-learning-la-gi/}},
  note         = {Ngày truy cập: 2023-12-13}
}
  
@misc{webpage12,
  title        = {Viblo - Word Embedding - Tìm hiểu khái niệm cơ bản trong NLP},
  author       = {{Bui Quang Manh}},
  year         = 2020,
  howpublished = {\url{https://viblo.asia/p/word-embedding-tim-hieu-khai-niem-co-ban-trong-nlp-1Je5E93G5nL}},
  note         = {Ngày truy cập: 2023-12-13}
}


@misc{webpage13,
  title        = {Word embedding là gì? Tại sao nó quan trọng?},
  year         = 2019,
  howpublished = {\url{https://trituenhantao.io/kien-thuc/word-embedding-la-gi-tai-sao-no-quan-trong/}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage14,
  title        = {Introduction to FastText Embeddings and its Implication},
  author       = {{Krithika V}, {Analytics Vidhya}},
  year         = 2023,
  howpublished = {\url{https://www.analyticsvidhya.com/blog/2023/01/introduction-to-fasttext-embeddings-and-its-implication/}},
  note         = {Ngày truy cập: 2023-12-13}
}

@misc{webpage15,
  title        = {200Lab - Google Colab là gì? Hướng dẫn sử dụng Google Colab},
  author       = {{Pum}},
  year         = 2022,
  howpublished = {\url{https://200lab.io/blog/google-colab-la-gi/}},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage16,
  title        = {Bizfly Cloud - TensorFlow là gì? Vai trò của TensorFlow trong sự phát triển của học máy},
  author       = {{Linh Tô}},
  year         = 2021,
  howpublished = {\url{https://bizflycloud.vn/tin-tuc/tensorflow-la-gi-vai-tro-cua-tensorflow-trong-su-phat-trien-cua-hoc-may-20211104172943825.htm}},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage17,
  title        = {GeeksforGeeks - Generating Word Cloud in Python},
  year         = 2021,
  howpublished = {\url{https://www.geeksforgeeks.org/generating-word-cloud-python/ }},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage18,
  title        = {TEK4.VN - Sử dụng Pickle để ghi file nhị phân},
  howpublished = {\url{https://tek4.vn/khoa-hoc/lap-trinh-python-can-ban/su-dung-pickle-de-ghi-file-nhi-phan}},
  note         = {Ngày truy cập: 2023-12-21}
}

@misc{webpage19,
  title        = {BERT Explained: A Complete Guide with Theory and Tutorial},
  author       = {{Samia Khalid}},
  year         = 2019,
  howpublished = {\url{https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage20,
  title        = {Bài 36 - BERT model},
  author       = {{Pham Dinh Khanh}},
  year         = 2020,
  howpublished = {\url{https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage21,
  title        = {BERT, RoBERTa, PhoBERT, BERTweet: Ứng dụng state-of-the-art pre-trained model cho bài toán phân loại văn bản},
  author       = {{Pham Huu Quang}},
  year         = 2020,
  howpublished = {\url{https://viblo.asia/p/bert-roberta-phobert-bertweet-ung-dung-state-of-the-art-pre-trained-model-cho-bai-toan-phan-loai-van-ban-4P856PEWZY3}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage22,
  title        = {Transformers - "Người máy biến hình" biến đổi thế giới NLP},
  author       = {{Nguyet Viet Anh}},
  year         = 2020,
  howpublished = {\url{https://viblo.asia/p/transformers-nguoi-may-bien-hinh-bien-doi-the-gioi-nlp-924lJPOXKPM}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage23,
  title        = {Bài 4 - Attention is all you need},
  author       = {{Pham Dinh Khanh}},
  year         = 2020,
  howpublished = {\url{https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage24,
  title        = {Natural language processing},
  year         = 2023,
  howpublished = {\url{https://en.wikipedia.org/wiki/Natural_language_processing}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage25,
  title        = {Toxic comments identification and classification using Deep Neural Networks},
  year         = 2023,
  howpublished = {\url{https://www.academia.edu/41458366/Toxic_Comments_Identification_and_Classification_Using_Deep_Neural_Networks}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage26,
  title        = {Top 8 Applications of Natural Language Processing (NLP)},
  year         = 2024,
  howpublished = {\url{https://eastgate-software.com/top-8-applications-of-natural-language-processing-nlp/}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage27,
  title        = {The Illustrated Transformer},
  year         = 2020,
  howpublished = {\url{https://jalammar.github.io/illustrated-transformer/}},
  note         = {Ngày truy cập: 2024-06-18}
}

@misc{webpage28,
  title        = {Việt Nam `lọt' top 5 ứng xử kém văn minh trên Internet},
  year         = 2020,
  howpublished = {\url{https://vtcnews.vn/viet-nam-lot-top-5-ung-xu-kem-van-minh-tren-internet-ar529256.html}},
  note         = {Ngày truy cập: 2024-06-18}
}
